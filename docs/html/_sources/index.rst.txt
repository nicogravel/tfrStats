**A mini-tutorial on the Statistical Assessment of Time Frequency Data**
########


Recently, I encountered an `article <https://www.tandfonline.com/doi/full/10.1080/08989621.2021.1962713>`_ discussing the ongoing replication issue in the field of biology. Why so often, the article stressed, results obtained by teams at different labs using the same data (and following the same inquiries) are difficult or even impossible to replicate reliably? According to the article, there is some agreement about what may be partly causing this replication crisis: in many cases statisticians act more like a plumber rather than a priest. While I personally do not agree with the dogmatic view of priest-like statisticians imposing their hypothesis-making machinery to every problem they stumble upon, I do agree on the need to reach some collective consensus, specially when potentially diverging decisions during the statistical assessment may bring forth confusion rather than clarity. Whit all its good intentions, the overly dogmatic approach of some scientists should not lead us into the realm of extreme scientific belief or `scientificism/scientism <https://www.merriam-webster.com/dictionary/scientism>`_ (i.e. *the urge to trust on the temporary answers our good ol' metric provide more than the underlying problem that inspired them in first place*). Coincidentally, while I was engrossed in this piece, I stumbled upon another noteworthy piece in the now obsolete Twitter. A `post <https://twitter.com/lakens/status/1718654122516156777>`_ provided the much needed, so zu sagen, plumber's perspective. After all, plumbing and fitting are nuanced tasks that can either yield a pipe jungle or a professionally designed system.

  *Statisticians should be less like priests and more plumbers. I don't care what you personally believe is the right way to do things - if I have a specific problem, I want to know all possible solutions that might fix it, what their limitations are, and how much each would cost.*                       `DaniÃ«l Lackens <https://twitter.com/lakens>`_

In this mini-tutorial, I show two related approaches applied to the same scenario. I provide basic Python code to illustrate how two different yet fundamentally similar pipelines can lead to slightly different but comparable results. The pipelines are based on examples provided in `Fieldtrip <https://www.fieldtriptoolbox.org/workshop/oslo2019/statistics/#permutation-test-with-cluster-correction>`_ and adapted from the `book <https://direct.mit.edu/books/book/4013/Analyzing-Neural-Time-Series-DataTheory-and>`_ Analyzing Neural Time Series Data: Theory and Practice. Specifically, to assess the statistical significance of spectral estimates obtained from electrophysiological data (in this case LFP) we used non-parametric permutation tests and focused on the multiple comparison correction of time frequency representations (TFRs). We show that the success of the two approaches depends on the dimensions of the data at hand, be these spatial locations, time and frequency analysis parameters, trials of different conditions, the nature of hypothesis, etc. For basic pedagogical purposes, here I focus on spectral power increases relative to baseline using two variants of essentially the same approach: 1) null hypothesis testing using the min-max distribution approach, which captures variations at the extremes of the null distribution, and 2) null hypothesis testing using the whole null distribution, obtained by averaging across specific dimensions.

Since there are several ways to achieve these goals and many realizations of these and other related methods (i.e. thresholds may be obtained from the percentiles of the null distribution directly and further corrected in equivalent ways, or the pooling of data accomplished among specific dimensions), here I focus on these two common methods using very simple examples in the hope to help those researchers (including myself) that are or may be in need of clarity on these matters, touch ground. Depending on the approach, the computation of the p-values will change slightly. In the min-max approach the minimum and maximum values at each permutations are used. When testing using the whole null distribution, the null values obtained from the permutations are averaged the same way the empirical distribution is averaged so the dimensions of the empirical distributions are preserved in the null distribution. Once the null distributions have been obtained, p-values are obtained using the empirical cumulative distribution. Provided the right Python environment, this Jupyter notebook should work as a simple mini-tutorial and support the discussion of these and related basic approaches for computing TFRs and assessing their statistical significance in a clear and easy to *understand/explain* way. The functions within this notebook are provided in an exploratory state and are subject to change. A background section with an open discussion subsection is included. Please feel free to use it help improve the codebook. The methods presented here are a work in progress. I plan to make some refinements in the next few weeks. Should you be interested in  employing these methods or if you have any questions, please feel free to reach out to me. I would be more than happy to assist you further.

The package can be downloaded from here: https://github.com/nicogravel/tfrStats


*****
Content
*****




.. toctree::
    :maxdepth: 3
    :numbered:
    :caption: Content:


    uvtfrs
    notebooks/statistical_approach

.. toctree::
    :maxdepth: 1
    :caption: Python package:

    modules
